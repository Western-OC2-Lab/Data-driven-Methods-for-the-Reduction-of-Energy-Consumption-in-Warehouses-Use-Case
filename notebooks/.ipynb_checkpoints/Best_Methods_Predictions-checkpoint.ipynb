{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847eb15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d407fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"../datasets/generated/2019_z2_Floor6.csv\", index_col=[0])\n",
    "testing_data = pd.read_csv(\"../datasets/generated/2018_z2_Floor6.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c08e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e32c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c27e72",
   "metadata": {},
   "source": [
    "# Method 2 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2898c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holiday = pd.read_csv(\"../datasets/Thailand_Holidays.csv\")\n",
    "df_holiday = df_holiday.loc[(df_holiday.Type == 'Government Holiday') | (df_holiday.Type == 'National Holiday'), :]\n",
    "df_holiday['month']= df_holiday['Date'].apply(lambda x : int(x.split(\"/\")[0]))\n",
    "df_holiday['day']= df_holiday['Date'].apply(lambda x : int(x.split(\"/\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_holiday(df_holiday, date):\n",
    "    day, month = int(date.day), int(date.month)\n",
    "    res = df_holiday.loc[(df_holiday.month == month) & (df_holiday.day == day)].shape[0]\n",
    "\n",
    "    return int(res > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bce7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.index = pd.to_datetime(testing_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(testing_data.index) - np.min(testing_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['output'] = training_data['z2_AC1(kW)']\n",
    "training_data.drop(columns = ['frame_id', 'z2_AC1(kW)'], inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_df = pd.DataFrame(np.c_[scaler.fit_transform(training_data.drop(columns = ['output'])), training_data.output.values], columns = training_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.index = training_data.index\n",
    "scaled_df.index = pd.to_datetime(scaled_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.loc[:, 'month'] = scaled_df.index.month\n",
    "scaled_df.loc[:, 'DoW'] = scaled_df.index.dayofweek\n",
    "scaled_df.loc[:, 'hour'] = scaled_df.index.hour\n",
    "scaled_df.loc[:, 'holiday'] = scaled_df.index.to_series().apply(lambda x: check_holiday(df_holiday, x))\n",
    "scaled_df = pd.get_dummies(scaled_df, columns = ['month', 'DoW', 'hour'], drop_first=True)\n",
    "scaled_df.index = range(scaled_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485eaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_shifted_df(sample_df, future):\n",
    "    new_df = sample_df.copy()\n",
    "    new_df.loc[:, 'output'] = new_df.loc[:, 'output'].shift(periods = -future)\n",
    "    \n",
    "    new_df.dropna(inplace=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9de8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bins = np.histogram(scaled_df.output.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d039a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69087bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum((scaled_df.output.values >= bins[6]).astype(int)) / scaled_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaled_df.output.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(pd.to_datetime(training_data.index).month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebe14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "future_values = [5, 10, 15, 20]\n",
    "alg_best = pd.DataFrame()\n",
    "unique_months = [f\"month_{m}\" for m in np.unique(pd.to_datetime(training_data.index).month)]\n",
    "_, bins = np.histogram(scaled_df.output.values)\n",
    "df_bins = pd.DataFrame()\n",
    "\n",
    "for f in tqdm(future_values):\n",
    "    dict_result = {}\n",
    "    dict_result['future'] = f\n",
    "    \n",
    "    shifted_df = produce_shifted_df(scaled_df, f)\n",
    "    \n",
    "    for month in tqdm(unique_months[1:]):\n",
    "        shifted_df_train, shifted_df_test = shifted_df.loc[shifted_df[month] == 0, :], shifted_df.loc[shifted_df[month] == 1, :]\n",
    "        X_train, y_train = shifted_df_train.drop(columns = ['output']), shifted_df_train.output.values\n",
    "        X_test, y_test = shifted_df_test.drop(columns = ['output']), shifted_df_test.output.values\n",
    "        \n",
    "        base_estimator = GradientBoostingRegressor(max_depth = 10, min_samples_split = 2)\n",
    "        base_estimator.fit(X_train, y_train)\n",
    "        \n",
    "        for bin_v in bins:\n",
    "            indices = np.where(y_test >= bin_v)[0]\n",
    "            if len(indices) > 0:\n",
    "                dict_result[f'testing_{month}_{bin_v}'] = mean_absolute_error(y_test[indices], base_estimator.predict(X_test.iloc[indices]))\n",
    "            else:\n",
    "                dict_result[f'testing_{month}_{bin_v}'] = \"N/A\"\n",
    "    \n",
    "    df_bins = pd.concat([df_bins, pd.DataFrame.from_dict([dict_result])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins.index = range(df_bins.shape[0])\n",
    "df_bins.to_csv(\"../results/stats/detailed_results_m2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf095281",
   "metadata": {},
   "source": [
    "# Method 3 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"../datasets/generated/2019_z2_Floor6.csv\", index_col=[0])\n",
    "testing_data = pd.read_csv(\"../datasets/generated/2018_z2_Floor6.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9759da",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.index = pd.to_datetime(training_data.index)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_frames = np.unique(training_data.frame_id.values)\n",
    "unique_months = np.unique(training_data.index.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['output'] = training_data[training_data.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_based_FE(df, lag_length, future, step):\n",
    "    new_df = df.copy()\n",
    "    for feature in new_df.columns:\n",
    "        if feature != \"output\":\n",
    "            for i in range(1, lag_length, step):\n",
    "                new_df.loc[:, f'{feature}_{i}'] = new_df.loc[:, feature].shift(periods = i)\n",
    "            new_df.drop(columns = [feature], inplace=True)\n",
    "        \n",
    "    new_df.loc[:, 'output'] = new_df.loc[:, 'output'].shift(periods = -future)\n",
    "    new_df.dropna(inplace=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f490b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_df_frames(all_df, lag_length, future, step, frames):\n",
    "    df = pd.DataFrame()\n",
    "    for frame in tqdm(frames):\n",
    "        df_frame = all_df.loc[all_df.frame_id == frame, :]\n",
    "        df_frame.drop(columns = ['frame_id'], inplace=True)\n",
    "        lagged_frame = lag_based_FE(df_frame, lag_length, future, step)\n",
    "        df = pd.concat([df, lagged_frame])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc09467",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_algorithms = {\n",
    "    5: GradientBoostingRegressor(max_depth = 3, min_samples_split = 10),\n",
    "    10: GradientBoostingRegressor(max_depth = 10, min_samples_split = 10),\n",
    "    15: GradientBoostingRegressor(max_depth = 10, min_samples_split = 10),\n",
    "    20: GradientBoostingRegressor(max_depth = 10, min_samples_split = 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33680b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bins = np.histogram(training_data.output.values, bins = 5)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "_, bins = np.histogram(training_data.output.values, bins = 5)\n",
    "\n",
    "set_hs_fs = [[5, 5], [10, 10],[15, 15],[20, 20]]\n",
    "alg_best_months = pd.DataFrame()\n",
    "\n",
    "\n",
    "for h_f in tqdm(set_hs_fs):\n",
    "    dict_result = {}\n",
    "    h, f = h_f[0], h_f[1]\n",
    "    dict_result['history'], dict_result['future'] = h, f\n",
    "    \n",
    "    for month in unique_months:\n",
    "        month_data = training_data.loc[training_data.index.month == month, :]\n",
    "        nonmonth_data = training_data.loc[training_data.index.month != month, :]\n",
    "        \n",
    "        training_frames, validation_frames = np.unique(nonmonth_data.frame_id.values), np.unique(month_data.frame_id.values)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        training_set, validation_set = produce_df_frames(nonmonth_data, h, f, 1, training_frames), produce_df_frames(month_data, h, f, 1, validation_frames)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        total_seq_time = round((end_time - start_time) /60, 2)\n",
    "        dict_result['preprocess_time'] = total_seq_time\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaled_train_df = pd.DataFrame(np.c_[training_set.output.values, scaler.fit_transform(training_set.drop(columns = ['output']))], columns = training_set.columns)\n",
    "        scaled_validation_df = pd.DataFrame(np.c_[validation_set.output.values, scaler.transform(validation_set.drop(columns = ['output']))], columns = training_set.columns)\n",
    "        \n",
    "        base_estimator = HPO_algorithms[h]\n",
    "        X_train, y_train = scaled_train_df.drop(columns  =['output']), scaled_train_df.output.values\n",
    "        X_validation, y_validation = scaled_validation_df.drop(columns  =['output']), scaled_validation_df.output.values\n",
    "        \n",
    "        start_time = time.time()\n",
    "        base_estimator.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        feature_imp_df = pd.DataFrame(base_estimator.feature_importances_, index = X_train.columns, columns = ['importance'])\n",
    "        \n",
    "        total_train_time = round((end_time - start_time) /60, 2)\n",
    "        dict_result['total_train_time'] = total_train_time\n",
    "        \n",
    "#         y_train_predictions = base_estimator.predict(X_train)\n",
    "#         y_validation_predictions = base_estimator.predict(X_validation)\n",
    "        \n",
    "        for idx, bin_v in enumerate(bins):\n",
    "            indices = np.where(y_validation >= bin_v)[0]\n",
    "            if len(indices) > 0:\n",
    "                dict_result[f'testing-{month}_bin-{idx}'] = mean_absolute_error(y_validation[indices], base_estimator.predict(X_validation.iloc[indices]))\n",
    "            else:\n",
    "                dict_result[f'testing-{month}_bin-{idx}'] = \"N/A\"\n",
    "                \n",
    "        feature_imp_df.to_csv(f\"../results/stats/stats_2019_z2_h-{h}_Floor6_m3_month-{month}_feature_imp.csv\")\n",
    "                \n",
    "    alg_best_months = pd.concat([alg_best_months, pd.DataFrame.from_dict([dict_result])])\n",
    "    alg_best_months.index = range(alg_best_months.shape[0])\n",
    "alg_best_months.to_csv(\"../results/stats/stats_2019_z2_Floor6_m3_months_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_best_months.to_csv(\"../results/stats/stats_2019_z2_Floor6_m3_months_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_frame_data = training_data.copy()\n",
    "removed_frame_data.drop(columns = ['frame_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_df = pd.DataFrame(np.c_[scaler.fit_transform(removed_frame_data.drop(columns = ['output'])), removed_frame_data.output.values], columns = removed_frame_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a40bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.index = pd.to_datetime(training_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.shape, scaled_df.shape, len(training_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f29390",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_months = np.unique(training_data.index.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d2c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea15035",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_hs_fs = [[5, 5], [10, 10],[15, 15],[20, 20]]\n",
    "df_bins = pd.DataFrame()\n",
    "\n",
    "for h_f in tqdm(set_hs_fs):\n",
    "    dict_result = {}\n",
    "    h, f = h_f[0], h_f[1]\n",
    "    dict_result['history'], dict_result['future'] = h, f\n",
    "    \n",
    "    for month in tqdm(unique_months[1:]):\n",
    "        scaled_df_train, scaled_df_test = scaled_df.loc[training_data.index.month == month, :], scaled_df.loc[training_data.index.month != month, :]\n",
    "        shifted_df_train, shifted_df_test  = lag_based_FE(scaled_df_train, h, f, 1), lag_based_FE(scaled_df_test, h, f, 1)\n",
    "        \n",
    "        X_train, y_train = shifted_df_train.drop(columns = ['output']), shifted_df_train.output.values\n",
    "        X_test, y_test = shifted_df_test.drop(columns = ['output']), shifted_df_test.output.values\n",
    "        \n",
    "        base_estimator = GradientBoostingRegressor(max_depth = 10, min_samples_split = 2)\n",
    "        base_estimator.fit(X_train, y_train)\n",
    "        \n",
    "        for bin_v in bins:\n",
    "            indices = np.where(y_test >= bin_v)[0]\n",
    "            if len(indices) > 0:\n",
    "                dict_result[f'testing_{month}_{bin_v}'] = mean_absolute_error(y_test[indices], base_estimator.predict(X_test.iloc[indices]))\n",
    "            else:\n",
    "                dict_result[f'testing_{month}_{bin_v}'] = \"N/A\"\n",
    "    \n",
    "    df_bins = pd.concat([df_bins, pd.DataFrame.from_dict([dict_result])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins.index = range(df_bins.shape[0])\n",
    "df_bins.to_csv(\"../results/stats/detailed_results_m3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd370b80",
   "metadata": {},
   "source": [
    "# 1D-CNN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(set_conv, dense_layers, n_timesteps,n_features, activation, lr):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=set_conv[0], kernel_size=3, activation=activation, input_shape=(n_timesteps,n_features)))\n",
    "\n",
    "    for conv_filter in set_conv[1:]:\n",
    "        model.add(Conv1D(filters=conv_filter, kernel_size=3, activation=activation))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate = lr)\n",
    "    for l in dense_layers:\n",
    "        model.add(Dense(l, activation))\n",
    "    model.add(Dense(1, activation))\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mean_absolute_error'], run_eagerly=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, out_steps, output_var):\n",
    "    X, y= list(), list()\n",
    "    for i in np.arange(start=0, stop=len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + (n_steps_in-1)\n",
    "        out_idx = end_ix + out_steps\n",
    "        if out_idx >= len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences.loc[i:end_ix, :]\n",
    "        seq_x.drop(columns = output_var, inplace=True)\n",
    "        seq_x = seq_x.values\n",
    "        seq_y = sequences.loc[out_idx, output_var]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"../datasets/generated/2019_z2_Floor6.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28190efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.index = pd.to_datetime(training_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['output'] = training_data['z2_AC1(kW)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d006679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_framewise_seq(sample_df, h, f, output_var):\n",
    "    unique_frames = np.unique(sample_df.frame_id)\n",
    "    first_frame = sample_df[sample_df.frame_id == unique_frames[0]]\n",
    "    first_frame.index = range(first_frame.shape[0])\n",
    "    \n",
    "    first_frame.drop(columns = ['frame_id'], inplace=True)\n",
    "\n",
    "    features, outputs = split_sequences(first_frame, h, f, output_var)\n",
    "#     print(f\"features: {features.shape}, outputs: {outputs.shape}\")\n",
    "    for i in tqdm(range(1, len(unique_frames))):\n",
    "        frame_df = sample_df[sample_df.frame_id == f\"frame_{i}\"]\n",
    "        frame_df.index = range(frame_df.shape[0])\n",
    "#         print('frame_df', frame_df.shape)\n",
    "        frame_df.drop(columns = ['frame_id'], inplace=True)\n",
    "        frame_X, frame_y = split_sequences(frame_df, h, f, output_var)\n",
    "#         print(f\"frame_X: {frame_X.shape}, frame_y: {frame_y.shape}\")\n",
    "        if frame_X.shape[0] != 0:\n",
    "            features, outputs = np.concatenate((features, frame_X)), np.concatenate((outputs, frame_y))\n",
    "        \n",
    "    return np.asarray(features).astype(np.float32), np.asarray(outputs).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b9cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_values = {\n",
    "    5: {'conv': [32], 'dense': [128]},\n",
    "    10: {'conv': [128], 'dense': [32, 64, 128]},\n",
    "    15: {'conv': [32, 64], 'dense': [32, 64]},\n",
    "    20: {'conv': [32, 64], 'dense': [32, 64, 128]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7330981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(training_data.index.month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac73ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "set_hs_fs = [[5, 5], [10, 10],[15, 15],[20, 20]]\n",
    "df_bins = pd.DataFrame()\n",
    "\n",
    "for h_f in tqdm(set_hs_fs):\n",
    "    dict_result = {}\n",
    "    h, f = h_f[0], h_f[1]\n",
    "    dict_result['history'], dict_result['future'] = h, f\n",
    "    \n",
    "    for month in tqdm(unique_months[1:]):\n",
    "        month_data = training_data.loc[training_data.index.month == month, :]\n",
    "        nonmonth_data = training_data.loc[training_data.index.month != month, :]\n",
    "        \n",
    "#         print(month_data.shape, nonmonth_data.shape, month)\n",
    "#         month_data = month_data.sample(n=20000)\n",
    "#         nonmonth_data = nonmonth_data.sample(n = 40000)\n",
    "        \n",
    "        month_data.index = range(month_data.shape[0])\n",
    "        nonmonth_data.index = range(nonmonth_data.shape[0])\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaled_df_train = pd.DataFrame(np.c_[scaler.fit_transform(nonmonth_data.drop(columns = ['frame_id', 'output'])), nonmonth_data.frame_id.values, nonmonth_data.output.values], columns = nonmonth_data.columns)\n",
    "        scaled_df_test = pd.DataFrame(np.c_[scaler.transform(month_data.drop(columns = ['frame_id', 'output'])), month_data.frame_id.values, month_data.output.values], columns = month_data.columns)\n",
    "        output_var = ['output']\n",
    "        \n",
    "        X_train, y_train = get_framewise_seq(scaled_df_train, h, f, output_var)\n",
    "        X_test, y_test = get_framewise_seq(scaled_df_train, h, f, output_var)\n",
    "        \n",
    "        hpo_values = optimal_values[h]\n",
    "        conv_values, dense_values = hpo_values['conv'], hpo_values['dense'] \n",
    "        \n",
    "        model = create_model(conv_values, dense_values, h,len(nonmonth_data.columns) - 2, 'linear', 1e-3)\n",
    "        print(X_train.shape, y_train.shape)\n",
    "        str_conv = \"_\".join([str(conv) for conv in conv_values])\n",
    "        str_dense = \"_\".join([str(conv) for conv in dense_values])\n",
    "\n",
    "        dict_result['conv'] = str_conv\n",
    "        dict_result['dense'] = str_dense\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train, verbose=0, batch_size=32, epochs = 5)\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_train_time = round((end_time - start_time) / 60, 2)\n",
    "        dict_result['total_train_time'] = total_train_time\n",
    "        \n",
    "        for bin_v in bins:\n",
    "            indices = np.where(y_test >= bin_v)[0]\n",
    "            if len(indices) > 0:\n",
    "                dict_result[f'testing_{month}_{bin_v}'] = mean_absolute_error(y_test[indices], model.predict(X_test[indices]))\n",
    "            else:\n",
    "                dict_result[f'testing_{month}_{bin_v}'] = \"N/A\"\n",
    "    \n",
    "    df_bins = pd.concat([df_bins, pd.DataFrame.from_dict([dict_result])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6571ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins.index = range(df_bins.shape[0])\n",
    "df_bins.to_csv(\"../results/stats/detailed_results_m1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
